{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bf606c",
   "metadata": {},
   "source": [
    "# Title  : Introduction to Natural Language Processing with spaCy\n",
    "### Author: Dr. Nimrita Koul. Associate Professor, Machine Learning, Bangalore, India.\n",
    "### Github:  \n",
    "### LinkedIn:\n",
    "\n",
    "#### References:\n",
    "1. https://spacy.io/usage/spacy-101\n",
    "2. https://course.spacy.io/en/\n",
    "3. https://github.com/explosion/spacy-course/tree/master/exercises/en\n",
    "3. https://www.machinelearningplus.com/spacy-tutorial-nlp/\n",
    "4. https://blog.dominodatalab.com/natural-language-in-python-using-spacy\n",
    "\n",
    "\n",
    "#### Pre-requisites for this attending this session: \n",
    "\n",
    "1. Intermediate knowledge of Python 3 programming and Jupyter Notebook application.\n",
    "2. Knowledge of libraries - pandas, numpy , sklearn including their installation using pip or conda\n",
    "3. Fundamentals of text processing like word to numerical vector representation. \n",
    "\n",
    "\n",
    "#### After attending this session, the participants will know the answers to these questions\n",
    "1. What is Natural Language Processing?\n",
    "2. What is spaCy - Architecture, Features and Functionality?\n",
    "3. How to do NLP tasks in spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed59af",
   "metadata": {},
   "source": [
    "### 1. What is Natural Language Processing?\n",
    "\n",
    "- A Natural Language is a language like English, Hindi, German etc. that is used by \"Humans\" to communicate with one another. \n",
    "\n",
    "\n",
    "- Natural language processing (NLP) is a branch of artificial intelligence that aims at building computer systems that can understand natural language and respond to it in natural language. \n",
    "\n",
    "- To \"understand\" means to recognize the meaning, the intent and the sentiment of the natural language text or speech. \n",
    "\n",
    "- NLP makes use of statistical models, machine learning and deep learning and helps you discover insights from your text data. It helps you anser question like:\n",
    "\n",
    "      1. What is the central idea of the text?\n",
    "      2. Who is doing what to whom?\n",
    "      3. Is the customer happy or upset about the product or service?\n",
    "      4. How similar are there two pieces of text?\n",
    "\n",
    "- It has applications in automated voice response systems, smart assistants like Alexa, chatbots, machine translation etc like Google Translate etc. \n",
    "\n",
    "####  Understanding text in human languages involves tasks like- \n",
    "        a. Identifying nouns, verbs i.e. parts of speech. \n",
    "        b. Identifying relationships among nouns and verbs. (Who is doing what to whom?)\n",
    "        c. Disambiguating meaning of a word in a context. \n",
    "        d. Entity Reference. (Which country the person belongs to? Who did it? What companies and products are mentioned?)\n",
    "        e. Sentiment Analysis. (Is the customer happy with the product?) \n",
    "\n",
    " \n",
    "### 2. [What is spaCy?](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "    - **spaCy** is a free, open-source, high speed library for **advanced** <u>Natural Language Processing (NLP)</u>  in Python.  \n",
    "    - spaCy is designed for building productions level NLP systems for information extraction, text processing and understanding large text volumes. \n",
    "    - spaCy supports 66+ languages and provides 76 trained NLP Pipelines/Models for 23 languages.\n",
    "    - Provides multi-task learning with pretrained transformers like BERT.\n",
    "    - Extensible with your own components and attributes, supports custom models in frameworks like PyTorch, TensorFlow.\n",
    "    \n",
    "    \n",
    "\n",
    "### 2a. Important Functionality of spaCy\n",
    "\n",
    "|NAME\t| DESCRIPTION| \n",
    "|:-----|:------------|\n",
    "|Tokenization\t| Segmenting text into words, punctuations marks etc.|\n",
    "|Part-of-speech (POS) Tagging |\tAssigning word types to tokens, like verb or noun.|\n",
    "|Dependency Parsing|\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
    "|Lemmatization|\tAssigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.|\n",
    "|Sentence Boundary Detection (SBD)|\tFinding and segmenting individual sentences.|\n",
    "|Named Entity Recognition (NER)|\tLabelling named “real-world” objects, like persons, companies or locations.|\n",
    "|Entity Linking (EL)|\tDisambiguating textual entities to unique identifiers in a knowledge base.|\n",
    "|Similarity|\tComparing words, text spans and documents and how similar they are to each other.|\n",
    "|Text Classification|\tAssigning categories or labels to a whole document, or parts of a document.|\n",
    "|Rule-based Matching|\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n",
    "|Training\t|Updating and improving a statistical model’s predictions.|\n",
    "|Serialization|\tSaving objects to files or byte strings.|\n",
    "\n",
    "\n",
    "### 2b. [Trained Statistical models in spaCy](https://spacy.io/usage/models)\n",
    "\n",
    "- spaCy provides various trained pipelines containing statistical models to predict linguistic annotations of your text. These linguistic annotations include information like whether a word is a noun or a verb, what is the relation among words in a sentence, which words refer to persons or other named entities etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3447f",
   "metadata": {},
   "source": [
    "## 3. Installing spaCy\n",
    "\n",
    "#### 3a. Install using Pip\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>It is recommended to create a separate virtual environment for installing spaCy:</b><br><br>\n",
    "     <code>python -m venv .env </code>  #create a new virtual env <br>\n",
    "     <code>source .env/bin/activate </code>  #activate new environment \n",
    "</div>\n",
    "\n",
    "\n",
    "#### Install Using pip\n",
    "\n",
    "<code>pip install -U pip setuptools wheel</code> #update pip, setuptools and wheel<br>\n",
    "\n",
    "<code>pip install spacy </code>                  # install spaCy   \n",
    "\n",
    "\n",
    "####  Download and install builtin Trained Pipeline for English Langauge(Small sized pipeline) via spaCy’s download command. \n",
    "It takes care of finding the best-matching package compatible with your spaCy installation.\n",
    "\n",
    "<code>python -m spacy download en_core_web_sm</code>\n",
    "\n",
    "\n",
    "#### 3b. Install using conda\n",
    "\n",
    "##### Create a new environment using conda  for spaCy  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>  Create a new Python environment with name \"myenv\":</b> <br><br>\n",
    "     <code>conda create --name myenv </code>  #create a new virtual env <br>\n",
    "     <code>sconda activate myenv </code>  #activate new environment <br>\n",
    "To relfect new environment in Jupyter Notebook, use following two line of code in this order at Ananconda Prompt:<br>\n",
    "<code>conda install -c anaconda ipykernel</code><br>\n",
    "<code>python -m ipykernel install --user --name= myenv</code><br>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Install spaCy in new environment Using conda : \n",
    "\n",
    "<code>conda install -c conda-forge spacy</code>  #use conda-forge channel<br>\n",
    "\n",
    "Download and install builtin Trained Pipeline for English Langauge(Small sized pipeline)\n",
    "\n",
    "<code>python -m spacy download en_core_web_sm</code>\n",
    "\n",
    "\n",
    "#### 3c. Updating spaCy\n",
    "\n",
    "<code>pip install -U spacy</code>\n",
    "\n",
    "##### Once installed, you will need to import spacy in your workspace\n",
    "\n",
    "<code>import spacy</code>\n",
    "\n",
    "###  3d. Download and install spaCy Trained Pipelines (Models) \n",
    "After installing spacy, you are required to install the trained pipelines/models.<br>\n",
    "Easiest method is to use spacy download command at command prompt, Anaconda prompt or in Jupyter Notebook:<br>\n",
    "\n",
    "<code>python -m spacy download en_core_web_sm</code> #Download and install small English language pipeline.\n",
    "\n",
    "#### Download and install builtin Trained Pipeline for English Langauge(Small sized pipeline) via spaCy’s download command. \n",
    "\n",
    "It takes care of finding the best-matching package compatible with your spaCy installation.\n",
    "\n",
    "<code>python -m spacy download en_core_web_sm</code>\n",
    "\n",
    "##### If you are running above line of code to download \"en_code_web_sm\" from Jupyter Notebook use below code:\n",
    "\n",
    "<code>!python -m spacy download en_core_web_sm</code>\n",
    "\n",
    "Other larger sized pipelines for English are called en_core_web_md, en_core_web_lg, en_core_web_trf. These differ in size and utility provided. Similarly spaCy has pipelines of different sizes for other languages as well.\n",
    "\n",
    "#### A spaCy trained Pipeline has components which are trained on labelled data to perform following NLP tasks:\n",
    "\n",
    "1. Tokenization\n",
    "2. Parts of speech tagging\n",
    "3. Name Entity Detection\n",
    "4. Dependency parsing\n",
    "5. Matching text with patterns\n",
    "\n",
    "Once you have downloaded and installed a pipeline e.g., \"en_core_web_sm\" you can load it in your workspace using **spacy.load()** method. This will return a **Language object** containing all components and data needed to process text. We usually call it **nlp**. Calling the nlp object on a string of text will return a processed **Doc** object.\n",
    "\n",
    "\n",
    "##### Names and size of some pipelines\n",
    "\n",
    "**\"en_core_web_sm\"**   is a small (12MB) English pipeline optimized for CPU. <br>\n",
    "Its components are: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.<br>\n",
    " \n",
    "Other English pipelines : <br>\n",
    "**en_core_web_md**  - medium sized model (31 MB) <br>\n",
    "**en_core_web_lg**  - large sized model  (382 MB)<br>\n",
    "**en_core_web_trf** - English transformer pipeline (483 MB) (roberta-base). <br>\n",
    "                      Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer.\n",
    "\n",
    "<br>\n",
    "**German Pipelines<br>  **\n",
    "**de_core_news_sm** - Small German pipeline optimized (13 MB) for CPU. <br>\n",
    "                      Components: tok2vec, tagger, morphologizer, parser, lemmatizer (trainable_lemmatizer), senter,ner.<br>\n",
    "**de_core_news_md**<br>\n",
    "**de_core_news_lg**<br>\n",
    "**de_core_news_trf**<br>\n",
    "\n",
    "Once downloaded, you can load langauage specific model with spacy.load() as shown below:<br>\n",
    "\n",
    "<code>\n",
    "import spacy\n",
    "# Load small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "</code>\n",
    "<br>\n",
    "Here nlp is a Langauge object with a lot of built-in text processing functionality. Also known as a **trained pipeline**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40a8bb",
   "metadata": {},
   "source": [
    "## 4. [Pipelines and Linguistic annotations provided by them](https://spacy.io/usage/spacy-101#annotations)\n",
    "\n",
    "\n",
    "### 4a.  [Pipelines](https://spacy.io/usage/spacy-101#pipelines)\n",
    "\n",
    "- When you run **spacy.load()** with the name of a trained language pipeline, a **Language object \"nlp\" is created**. \n",
    "- When you call the nlp object on your text, it does a series of operation on it. The operations depend on the components in the trained pipeline. The series of operations is known as a **processing pipeline**. \n",
    "- Typical components in a trained pipeline are - **Tokenizer, POS tagger, lemmatizer, parser, and Entity Recognizer**. \n",
    "- **Tokenizer** splits your text into tokens and create a **Doc** object. Then the next component taken this Doc as input, performs its operation and passes the processed Doc to the next component. \n",
    "- Trained components of the pipeline contain the model as well as the model weights to make predictions. Each pipeline specifies its components and their settings in the config module.\n",
    "\n",
    "\n",
    "<img src = \"https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\" width = 800>\n",
    "<center><i>Source: \"https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\" </i></center>\n",
    "\n",
    "\n",
    "\n",
    "### 4b.  Linguistic annotations provided by the trained pipelines\n",
    "- spaCy pipelines provide langauge specific information (linguistic annotations) about your text. This information is stored as linguistic annotations of the tokens in your text. \n",
    "- This information includes the word types i.e. the part of speech a word is (noun, verb, pronoun etc.), relation between words (subject, object etc.), whether a word is the name of a person, a company or a geographical location etc.\n",
    "\n",
    "  \n",
    "#### 1. Linguistic annotations: Tokenization\n",
    "First step in processing of text is Tokenization. I.e., splitting larger chunks of text into smaller segments as per language specific rules. E.g., sentences into words and punctuation, paragraphs into sentences. Tokenization in spaCy can be done even without a trained pipeline. \n",
    "\n",
    "After tokenization step, rest all linguistic annotations require a trained model. \n",
    "\n",
    "<code>\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello, My Name is Nimrita\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "</code>\n",
    "\n",
    "#### 2. Linguistic annotations: Part-of-speech tags and dependencies \n",
    "- After tokenization, spaCy uses the trained components of pipeline in the nlp object to parse the doc object and predict the tags for the tokens in a Doc object. \n",
    "- A trained component has been trained using supervised learning and is capable of making predictions that generalize across the language. The binary data of these trained models is present in the pipeline. \n",
    "- All the linguistic annotations are available as Token attributes. \n",
    "- spaCy encodes all text strings to **hash values**. This helps reduce memory usage and improves efficiency. So to print or read the string representation of an attribute, addan underscore _ to its name. E.g. to see part of speech tag of a token in a Doc object use **token.pos_**.\n",
    "\n",
    "\n",
    "#### 3. Linguistic annotations: Named Entities \n",
    "\n",
    "- Real world objects like a country, a book title, a person, a company that is assigned a name is known as a **\"Named Entity\"**. \n",
    "\n",
    "- spaCy pipeline's **Named Entity Recognizer** component can predict the Named Entity types for text in a document. \n",
    "- Named entities are available as the **ents** property of a Doc object.\n",
    "\n",
    "\n",
    "#### 3. Linguistic annotations: Word vectors and similarity \n",
    "- Machine learning/deep learning or statistical models can be trained on numerical data. Therefore, the text strings are converted into numerical vectors known as **Word Vectors** or **word embeddings**. These are multi-dimensional representations of the meaning of a word. Algorithms like **word2vec** are used to generate these word vectors. \n",
    "\n",
    "- Small sized trained pipelines for any langauge do not contain word vectors. To use word vectors please download and install medium, large or transformer pipelines. \n",
    "\n",
    "<code>!python -m spacy download en_core_web_lg</code>\n",
    "\n",
    "- These pipeline packages let you use Token.vector, Doc.vector or Span.vector attributes. Doc.vector and Span.vector will default to an average of their token vectors. \n",
    "\n",
    "- spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one.\n",
    "\n",
    "- Similarity among words is found by comparing their word vectors. It is a floating point number between 0 and 1. Doc, Span, Token and Lexeme comes with a .similarity method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bc53a",
   "metadata": {},
   "source": [
    "## 5. [Architecture of spaCy](https://spacy.io/usage/spacy-101#architecture)\n",
    "\n",
    "Three main data structures in spaCy are \n",
    "\n",
    "      1. Language Class or the nlp Object - the nlp object is created by spacy.load() or spacy.blank(). \n",
    "         It processes text and returns a Doc object.\n",
    "      2. Vocab Object -  The shared vocabulary that stores strings.\n",
    "      3. Doc Object -contains and owns the tokens and their annotations. \n",
    "\n",
    "- The Language object (nlp) takes the raw text and sends it through the pipeline, returning an annotated document. It also orchestrates training and serialization.\n",
    "\n",
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/f35bab3fa9d1e91601695dbd6241a607ba11876c/179f6/architecture-415624fc7d149ec03f2736c4aa8b8f3c.svg\" width = 600/>\n",
    "\n",
    "<center><i>Architecture of spaCy.  Source: https://spacy.io/usage/spacy-101</i></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5a. Container objects in spaCy\n",
    "| NAME |\tDESCRIPTION |\n",
    "|:-----|:------------|\n",
    "| Doc |\tA container for accessing linguistic annotations.|\n",
    "| DocBin |\tA collection of Doc objects for efficient binary serialization. Also used for training data.|\n",
    "| Example |\tA collection of training annotations, containing two Doc objects: the reference data and the predictions.|\n",
    "| Language |\tProcessing class that turns text into Doc objects. The variable is typically called nlp.|\n",
    "| Lexeme |\tAn entry in the vocabulary. It’s a word type with no context, as opposed to a word token. It has no part-of-speech tag, dependency parse etc.|\n",
    "| Span |\tA slice from a Doc object.|\n",
    "| SpanGroup |\tA named collection of spans belonging to a Doc.|\n",
    "| Token\t|An individual token — i.e. a word, punctuation symbol, whitespace, etc.|\n",
    "\n",
    "##### Tokens\n",
    "Tokens are individual text entities like words, punctuation, spaces, etc.\n",
    "\n",
    "**Tokenization** is the first step in many NLP tasks. It means to break text into smaller constituent tokens. E.g., sentences into words, punctuation, paragraphs into sentences etc. Different kinds of tokens have different type of information associated with them by spaCy. This information can be accessed as lexical attributes of these tokens.\n",
    "\n",
    "\n",
    "##### Lexical attributes of spaCy Tokens\n",
    "Some common lexical attributes of Token object are: is_punct, is_ascii, is_digit, is_bracket, is_quote, is_upper, is_lower, like_url etc.\n",
    "token.like_num : True if the token is a number\n",
    "token.is_alpha : Returns True if the token is an alphabet\n",
    "token.is_digit : Returns True if the token is a number(0-9)\n",
    "token.is_upper : Returns True if the token is upper case alphabet\n",
    "token.like_url : Returns True if the token is a URL\n",
    "\n",
    "\n",
    "### 5b. Processing Pipeline in spaCy\n",
    "\n",
    "- The components of the trained model are applied on the Doc object in a specific order. E.g., Tokenizer is run first of all other components. This series of processing steps forms the processing pipeline. \n",
    "\n",
    "- We can add more built-in components or our own custom components to a Langauge object (nlp) using nlp.add_pipe() method. \n",
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\" width = 800/>\n",
    "<center><i> Typical Pipeline Components. Source: https://spacy.io/usage/spacy-101 </i></center>\n",
    " \n",
    "There are other built-in spacy components that can be added to a pipeline if your task requires so - AttributeRuler, DependencyParser, EditTreeLemmatizer, Morphologizer, SentenceRecognizer,Tok2Vec, Transformer and more. \n",
    "\n",
    "\n",
    "### 5c. Matchers\n",
    "Matchers help you find and extract matches between patterns of text and the text in a Doc object. \n",
    "\n",
    "\n",
    "### 5d. [Other classes in spaCy](https://spacy.io/usage/spacy-101#architecture-other)\n",
    "\n",
    "Other classes in spaCy are Corpus Class, KnowledgeBase, Lookups, MorphAnalys, Morphology, Scorer, StringStore, Vectors, Vocab. \n",
    "\n",
    "\n",
    "### 5e. [Vocab, hashes and lexemes](https://spacy.io/usage/spacy-101#vocab)\n",
    "\n",
    "    -  The **Vocab** object in spaCy is a shared Vocabulary that stores the text data in many documents. \n",
    "       It uses a hash function to convert text strings, entity labels, part of speech tags etc. into numerical hash values. \n",
    "\n",
    "       For example, the word \"coffee\" has the hash 3197928453018144401. So if you have many documents that contain the word    \"coffee\", spaCy hashes the word and stores it only once in the **StringStore**. The StringStore is a lookup table in which you can look up a string to get its hash, or a hash to get its string.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<code>\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'\n",
    "</code>\n",
    "</div>\n",
    "\n",
    "      - Each entry in Vocabulary is called a **Lexeme**. Vocabulary may not include word text, and can look up it up in the StringStore from the hashvalue.  A Lexeme contains the context-independent information about a word. For example, spelling, nature of characters in the word etc. i.e. The information that won't change based on context. Hashes are irreversible, i.e., we cannot generate the text string from a hash value, we can only look them up in vocabulary.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<code>\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)\n",
    "</code>\n",
    "\n",
    "Here: <br>\n",
    "Text: The original text of the lexeme. <br>\n",
    "Orth: The hash value of the lexeme. <br>\n",
    "Shape: The abstract word shape of the lexeme. <br>\n",
    "Prefix: By default, the first letter of the word string. <br>\n",
    "Suffix: By default, the last three letters of the word string. <br>\n",
    "is alpha: Does the lexeme consist of alphabetic characters? <br>\n",
    "is digit: Does the lexeme consist of digits? <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45967cb",
   "metadata": {},
   "source": [
    "## 6. [Serialization - Saving your spaCy projects](https://spacy.io/usage/spacy-101#serialization)\n",
    "\n",
    "You can save your spaCy projects or the customization you do to the spaCy pipeline components by converting them into a byte string. This process is called as serialization. spaCy uses Python's builtin Pickle protocol to serialize your objects and to save them to hard disk or to load them from hard disk. All container classes in spaCy - Language (nlp), Doc, Vocab and StringStore can be serialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705af3f",
   "metadata": {},
   "source": [
    "## 7. [Training (Fine Tuning) the Components of spaCy pipeline](https://spacy.io/usage/spacy-101#training)\n",
    "\n",
    "- Components of spaCy's pipeline - POS tagger, parser, text categorizer etc. are statistical models that have been trained using supervised machine learning (Backpropagation and Gradient descent). The model weights are provided as a part of the pipeline. However, some of these components can be fine tuned with your own data or your own annotations. For this, you can use spacy train() method with configuration settings of your choice. \n",
    "\n",
    "#### Trainable components\n",
    "\n",
    "- **Pipe** class in spaCy allows you to create your own components/models to make predictions of your choice on the Doc objects. These components can be tained using train(). You can use these components as part of the spaCy pipeline. \n",
    "\n",
    "#### Training config and lifecycle\n",
    "\n",
    "- Pipeline contains **Training config file** called as **config.cfg** that include all settings and hyperparameters for training your pipeline. You can call train() method and pass the training configuration by writing it in the config.cfg file. There is no need to provide the training parameters on the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d4909",
   "metadata": {},
   "source": [
    "## 8. [Language data](https://spacy.io/usage/spacy-101#language-data)\n",
    "\n",
    "The **lang** module of spaCy contains language specific data that is organized in simple Python files. It includes data like which words are very common, what are the exceptions or special cases of word usage in that language. The root directory contains shared language data that has the rules for basic punctuation , emoji, emoticons, single letter abbreviations that can be shared across languages. Subdirectories/submodules contain language specific rules. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<code>\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "nlp_en = English()  # Includes English data\n",
    "nlp_de = German()  # Includes German data\n",
    "</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231370fc",
   "metadata": {},
   "source": [
    "### Hands-on Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789f36a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "# import spacy into your workspace and print the version of spacy you have\n",
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c94f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "My\n",
      "Name\n",
      "is\n",
      "Nimrita\n"
     ]
    }
   ],
   "source": [
    "## Load small English pipeline and apply it to your text. Print the tokens of your text\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hello, My Name is Nimrita\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b2dec",
   "metadata": {},
   "source": [
    "#### [Lemmatization](https://spacy.io/usage/linguistic-features)\n",
    "\n",
    "Lemmatization is the process of identifying the root word from a group of related words. \n",
    "E.g. 'ate', 'eat', 'eating' all are derived from the word 'eat'. \n",
    "Buy, bought, buying, buyer all are derived from the word 'buy'. \n",
    "This helps in correct interpretation of meaning of the text and reduces the overall amount of text to tbe processed by NLP models. **Token.lemma_** attribute helps you identigy the lemma of a token in spaCy.\n",
    "\n",
    "\n",
    "#### [Dependency Parsing](https://spacy.io/usage/linguistic-features#dependency-parse)\n",
    "\n",
    "[The Stanford NLP Group](https://nlp.stanford.edu/software/nndep.html) defines the task of a dependency parser as follows:<br>\n",
    "\n",
    "[A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads. The figure below shows a dependency parse of a short sentence. The arrow from the word moving to the word faster indicates that faster modifies moving, and the label advmod assigned to the arrow describes the exact nature of the dependency](https://nlp.stanford.edu/software/nndep.html)\n",
    "\n",
    "<img src = \"https://nlp.stanford.edu/static/img/nndep-example.png\" width=\"1500\"/>\n",
    "<center><i>Source Credits: https://nlp.stanford.edu/static/img/nndep-example.png</i></center>\n",
    "\n",
    "token.dep_ attribute can be used to know the dependency relationship of a token to other tokens in spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7504135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text: Apple      Lemma: Apple     Simple POS: PROPN   Dependency: nsubj      Shape: Xxxxx    Is Alphabet:  1 Stopword:  0\n",
      "Token Text: buys       Lemma: buy       Simple POS: VERB    Dependency: ROOT       Shape: xxxx     Is Alphabet:  1 Stopword:  0\n",
      "Token Text: U.K.       Lemma: U.K.      Simple POS: PROPN   Dependency: compound   Shape: X.X.     Is Alphabet:  0 Stopword:  0\n",
      "Token Text: startup    Lemma: startup   Simple POS: NOUN    Dependency: dobj       Shape: xxxx     Is Alphabet:  1 Stopword:  0\n",
      "Token Text: for        Lemma: for       Simple POS: ADP     Dependency: prep       Shape: xxx      Is Alphabet:  1 Stopword:  1\n",
      "Token Text: $          Lemma: $         Simple POS: SYM     Dependency: quantmod   Shape: $        Is Alphabet:  0 Stopword:  0\n",
      "Token Text: 1          Lemma: 1         Simple POS: NUM     Dependency: compound   Shape: d        Is Alphabet:  0 Stopword:  0\n",
      "Token Text: billion    Lemma: billion   Simple POS: NUM     Dependency: pobj       Shape: xxxx     Is Alphabet:  1 Stopword:  0\n"
     ]
    }
   ],
   "source": [
    "# Print token text, lemma, simple pos tag, detailed POS tag, syntactic dependency, \n",
    "#shape, and some lexical attributes of tokens \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple buys U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(f\"Token Text: {token.text:10} Lemma: {token.lemma_:9} Simple POS: {token.pos_:6}  Dependency: {token.dep_:10} Shape: {token.shape_:8} Is Alphabet: {token.is_alpha:2} Stopword: {token.is_stop:2}\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd927d",
   "metadata": {},
   "source": [
    "#### Print all Named Entities in text\n",
    "Named Entities in text are the words that represent real world entities that have a name. For example, a person, a country, a company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49eb472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Text: Apple      Entity Label: ORG       \n",
      "Entity Text: U.K.       Entity Label: GPE       \n",
      "Entity Text: $1 billion Entity Label: MONEY     \n"
     ]
    }
   ],
   "source": [
    "# Print all Named Entities in text\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple buys U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity Text: {ent.text:10} Entity Label: {ent.label_:10}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbadd3",
   "metadata": {},
   "source": [
    "#### spacy.explain() to understand the tags you don't follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1b442e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using spacy.explain() function , you can know the explanation or full-form of any Term that you may not know of.\n",
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe40ba",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity \n",
    "Word vectors are numerical representations of the meaning of words. Since statistical models work with numbers so all text needs to be represented in the form of numerical vectors. Common algorithms like Word2Vec or Token2vec generate such word embeddings or vector representations from text. Vectors for Doc are average of the vectors for vectors of its tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9757578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text: dog      Has Vector:  1 Vector L2 Norm:  7.443447113037109 Out of Vocabulary: False\n",
      "Token Vector \n",
      ": [-0.72483    0.42538    0.025489  -0.39807    0.037463  -0.29811\n",
      " -0.28279    0.29333    0.57775    1.2205    -0.27903    0.80879\n",
      " -0.71291    0.045808  -0.46751    0.55944    0.42745    0.58238\n",
      "  0.20854   -0.42718   -0.40284   -0.048941   0.1149    -0.6963\n",
      " -0.03338    0.052596  -0.22572   -0.35996    0.47961   -0.38386\n",
      " -0.73837    0.1718     0.52188    0.45584   -0.026621   0.48831\n",
      "  0.67996   -0.73345   -0.27078    0.41739    0.1947     0.27389\n",
      " -0.70931   -0.45317   -0.22574   -0.12617    0.03268    0.142\n",
      "  0.53923   -0.61285   -0.5322     0.19479    0.13889   -0.020284\n",
      "  0.088162   0.85337    0.039407   0.11529   -0.42646    0.74832\n",
      "  0.34421   -0.59462    0.0040537  0.027203  -0.063394   0.26538\n",
      "  0.34757    0.21395   -0.39799   -0.027067  -0.36132    0.31979\n",
      "  0.55813   -0.5652     0.55382    0.03928   -0.26933   -0.14705\n",
      "  0.74032   -0.50566    0.023765   0.62273   -0.79388   -0.25165\n",
      "  0.11992   -0.43056    1.0614     0.58571    0.8856    -0.056054\n",
      "  0.055826   0.30485    0.64639   -0.43831   -0.45706    0.036471\n",
      " -0.3466    -0.56219    0.28105   -0.33758   -0.041398   0.22171\n",
      "  0.05262    0.18113    0.65646   -0.56217    0.038915  -0.30335\n",
      "  0.05051   -0.2354     0.3233     0.31744    0.52453   -0.47154\n",
      "  0.13152   -0.15104    0.14265   -0.20747    0.060413  -0.030342\n",
      " -0.092883   0.80421   -0.12497   -0.56199    0.29128   -0.22488\n",
      "  0.30282   -0.0045144 -0.12305    0.20396   -0.32202   -0.11409\n",
      " -0.37613    0.40457    0.21461    0.25741   -0.36489    0.94135\n",
      "  0.42725    0.022925  -1.8699    -0.76035    0.73771    0.36998\n",
      "  0.50214   -0.30617   -0.26526    0.86573    0.3808     0.14754\n",
      "  0.29932   -0.078863  -0.28992   -0.064636  -0.68914    0.19527\n",
      " -0.56368    0.26251   -0.52171   -1.0703     0.42478   -0.0067289\n",
      " -0.28591   -0.77831    0.049342   0.66675   -0.077419  -0.19226\n",
      "  0.12721   -0.18844    0.13647    0.38804    0.21917   -0.24192\n",
      " -0.13465    0.23119   -0.43197    0.48302    0.3598     1.128\n",
      "  0.019894  -0.10861   -0.13515   -0.34137   -0.36379    0.080616\n",
      "  0.28682   -0.045819  -0.12114   -0.44835   -0.054611  -0.10362\n",
      "  0.010954  -0.60063   -0.46665    0.15115   -0.31815   -0.58903\n",
      "  1.1325     0.04406   -0.92863    0.3399    -0.03463   -0.40474\n",
      "  0.17245   -0.19983   -0.095982  -0.074758   0.57472    0.25455\n",
      " -0.20387    0.055758  -0.65017    0.72629   -0.51083    0.11196\n",
      "  0.44724    0.16157   -0.34571    0.19227   -0.063871   0.0057351\n",
      "  0.48703   -0.53762   -0.73398   -0.11488    0.073723   0.58191\n",
      "  0.33192   -0.13303   -0.3478    -0.022676  -0.32494   -0.26496\n",
      "  0.56275    0.098558  -0.16671   -0.40481    0.55477   -0.58692\n",
      " -0.60433   -0.4227    -0.53712    0.2994     0.11339   -0.3154\n",
      " -0.28685    0.43999    0.013623   0.011139  -0.47734   -0.01492\n",
      "  0.52524    0.53583    0.36626    0.23119   -0.1386     0.35374\n",
      " -0.27448    0.066183   0.6224    -0.24851   -0.36066    0.009084\n",
      " -0.58148    0.24371    0.29944   -0.025314  -0.73222    0.33236\n",
      " -0.40339    0.82624    0.006984   0.26737   -0.27695   -0.09713\n",
      " -0.015736   0.1024    -0.026831  -0.26293    0.31401    0.01051\n",
      " -0.048451  -0.74571    0.75827    0.67771    0.054738  -0.23325\n",
      "  0.17996   -0.206      0.019095  -0.34283   -0.58602    0.0095634\n",
      " -0.085052   0.83312    0.31978    0.050317  -0.23159    0.28165  ]\n",
      "Token Text: cat      Has Vector:  1 Vector L2 Norm:  7.443447113037109 Out of Vocabulary: False\n",
      "Token Vector \n",
      ": [-0.72483    0.42538    0.025489  -0.39807    0.037463  -0.29811\n",
      " -0.28279    0.29333    0.57775    1.2205    -0.27903    0.80879\n",
      " -0.71291    0.045808  -0.46751    0.55944    0.42745    0.58238\n",
      "  0.20854   -0.42718   -0.40284   -0.048941   0.1149    -0.6963\n",
      " -0.03338    0.052596  -0.22572   -0.35996    0.47961   -0.38386\n",
      " -0.73837    0.1718     0.52188    0.45584   -0.026621   0.48831\n",
      "  0.67996   -0.73345   -0.27078    0.41739    0.1947     0.27389\n",
      " -0.70931   -0.45317   -0.22574   -0.12617    0.03268    0.142\n",
      "  0.53923   -0.61285   -0.5322     0.19479    0.13889   -0.020284\n",
      "  0.088162   0.85337    0.039407   0.11529   -0.42646    0.74832\n",
      "  0.34421   -0.59462    0.0040537  0.027203  -0.063394   0.26538\n",
      "  0.34757    0.21395   -0.39799   -0.027067  -0.36132    0.31979\n",
      "  0.55813   -0.5652     0.55382    0.03928   -0.26933   -0.14705\n",
      "  0.74032   -0.50566    0.023765   0.62273   -0.79388   -0.25165\n",
      "  0.11992   -0.43056    1.0614     0.58571    0.8856    -0.056054\n",
      "  0.055826   0.30485    0.64639   -0.43831   -0.45706    0.036471\n",
      " -0.3466    -0.56219    0.28105   -0.33758   -0.041398   0.22171\n",
      "  0.05262    0.18113    0.65646   -0.56217    0.038915  -0.30335\n",
      "  0.05051   -0.2354     0.3233     0.31744    0.52453   -0.47154\n",
      "  0.13152   -0.15104    0.14265   -0.20747    0.060413  -0.030342\n",
      " -0.092883   0.80421   -0.12497   -0.56199    0.29128   -0.22488\n",
      "  0.30282   -0.0045144 -0.12305    0.20396   -0.32202   -0.11409\n",
      " -0.37613    0.40457    0.21461    0.25741   -0.36489    0.94135\n",
      "  0.42725    0.022925  -1.8699    -0.76035    0.73771    0.36998\n",
      "  0.50214   -0.30617   -0.26526    0.86573    0.3808     0.14754\n",
      "  0.29932   -0.078863  -0.28992   -0.064636  -0.68914    0.19527\n",
      " -0.56368    0.26251   -0.52171   -1.0703     0.42478   -0.0067289\n",
      " -0.28591   -0.77831    0.049342   0.66675   -0.077419  -0.19226\n",
      "  0.12721   -0.18844    0.13647    0.38804    0.21917   -0.24192\n",
      " -0.13465    0.23119   -0.43197    0.48302    0.3598     1.128\n",
      "  0.019894  -0.10861   -0.13515   -0.34137   -0.36379    0.080616\n",
      "  0.28682   -0.045819  -0.12114   -0.44835   -0.054611  -0.10362\n",
      "  0.010954  -0.60063   -0.46665    0.15115   -0.31815   -0.58903\n",
      "  1.1325     0.04406   -0.92863    0.3399    -0.03463   -0.40474\n",
      "  0.17245   -0.19983   -0.095982  -0.074758   0.57472    0.25455\n",
      " -0.20387    0.055758  -0.65017    0.72629   -0.51083    0.11196\n",
      "  0.44724    0.16157   -0.34571    0.19227   -0.063871   0.0057351\n",
      "  0.48703   -0.53762   -0.73398   -0.11488    0.073723   0.58191\n",
      "  0.33192   -0.13303   -0.3478    -0.022676  -0.32494   -0.26496\n",
      "  0.56275    0.098558  -0.16671   -0.40481    0.55477   -0.58692\n",
      " -0.60433   -0.4227    -0.53712    0.2994     0.11339   -0.3154\n",
      " -0.28685    0.43999    0.013623   0.011139  -0.47734   -0.01492\n",
      "  0.52524    0.53583    0.36626    0.23119   -0.1386     0.35374\n",
      " -0.27448    0.066183   0.6224    -0.24851   -0.36066    0.009084\n",
      " -0.58148    0.24371    0.29944   -0.025314  -0.73222    0.33236\n",
      " -0.40339    0.82624    0.006984   0.26737   -0.27695   -0.09713\n",
      " -0.015736   0.1024    -0.026831  -0.26293    0.31401    0.01051\n",
      " -0.048451  -0.74571    0.75827    0.67771    0.054738  -0.23325\n",
      "  0.17996   -0.206      0.019095  -0.34283   -0.58602    0.0095634\n",
      " -0.085052   0.83312    0.31978    0.050317  -0.23159    0.28165  ]\n",
      "Token Text: banana   Has Vector:  1 Vector L2 Norm:   6.89589786529541 Out of Vocabulary: False\n",
      "Token Vector \n",
      ": [-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
      " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
      " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
      " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
      "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
      " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
      "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
      " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
      "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
      "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
      "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
      " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
      "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
      " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
      " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
      "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
      " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
      "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
      "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
      " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
      " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
      "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
      "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
      " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
      " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
      "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
      " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
      "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
      "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
      " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
      "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
      " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
      "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
      " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
      "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
      " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
      " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
      "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
      " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
      " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
      " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
      " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
      " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
      "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
      " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
      " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
      "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
      " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
      " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
      " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n",
      "Token Text: afskfsd  Has Vector:  0 Vector L2 Norm:                0.0 Out of Vocabulary: True\n",
      "Token Vector \n",
      ": [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "Similarity of doc1[0] with doc1[1]: 1.0000001192092896 \n",
      "\n",
      "\n",
      "Similarity of doc1 with doc2: 0.7071206680444951 \n",
      "\n",
      "\n",
      "Similarity of doc1 with doc3: 0.3090511739579275 \n",
      "\n",
      "\n",
      "Similarity of doc1 with doc4: 0.9685581106439412 \n"
     ]
    }
   ],
   "source": [
    "## Print word vectors and similarity between words\n",
    "# Small pipelines dont contain word vectors, so we have downloaded and installed medium size English pipeline for this code\n",
    "# Similarity is obtained using the .similarity() method that can be applied on Doc, Span, View objects.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text1 = \"dog cat banana afskfsd\"\n",
    "doc1 = nlp(text1)\n",
    "for token in doc1:\n",
    "    print(f\"Token Text: {token.text:8} Has Vector: {token.has_vector:2} Vector L2 Norm: {token.vector_norm:18} Out of Vocabulary: {token.is_oov}\")\n",
    "    print(f\"Token Vector \\n: {token.vector}\")\n",
    "\n",
    "    \n",
    "text2 = \"i have a dog and a cat\"\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "text3 = \"i love rain\"\n",
    "doc3 = nlp(text3)\n",
    "\n",
    "text4 = \"cat banana\"\n",
    "doc4 = nlp(text4)\n",
    "\n",
    "print(f'\\n\\nSimilarity of doc1 with doc2: {doc1.similarity(doc2)} ')\n",
    "\n",
    "print(f'\\n\\nSimilarity of doc1 with doc3: {doc1.similarity(doc3)} ')\n",
    "\n",
    "print(f'\\n\\nSimilarity of doc1 with doc4: {doc1.similarity(doc4)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12443c9f",
   "metadata": {},
   "source": [
    "The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of 0, which means it’s practically nonexistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305a5a8",
   "metadata": {},
   "source": [
    "### Text-Cleaning or Preprocessing with spaCy\n",
    "\n",
    "- Text cleaning or preprocessing involves removing the parts of text that do not add value to the meaning to the text. \n",
    "- Most common examples of these are the words like “a”, ” the”, “was”, \"it\" etc. that do not add value to the meaning of    the text. \n",
    "- Such words are known as **Stopwords**. \n",
    "- Another group of text which can be removed is the punctuation marks. \n",
    "- spaCy tokens have attributes that tell you if a word is noise. \n",
    "- E.g. some attributes are  : token.is_stop, token.is_punct, token.is_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4e93fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Text: I          Stop word?:  1 Punctuation?:  0\n",
      "Token Text: love       Stop word?:  0 Punctuation?:  0\n",
      "Token Text: to         Stop word?:  1 Punctuation?:  0\n",
      "Token Text: read       Stop word?:  0 Punctuation?:  0\n",
      "Token Text: a          Stop word?:  1 Punctuation?:  0\n",
      "Token Text: book       Stop word?:  0 Punctuation?:  0\n",
      "Token Text: on         Stop word?:  1 Punctuation?:  0\n",
      "Token Text: a          Stop word?:  1 Punctuation?:  0\n",
      "Token Text: beach      Stop word?:  0 Punctuation?:  0\n",
      "Token Text: .          Stop word?:  0 Punctuation?:  1\n",
      "Token Text: The        Stop word?:  1 Punctuation?:  0\n",
      "Token Text: breeze     Stop word?:  0 Punctuation?:  0\n",
      "Token Text: is         Stop word?:  1 Punctuation?:  0\n",
      "Token Text: relaxing   Stop word?:  0 Punctuation?:  0\n",
      "Token Text: !          Stop word?:  0 Punctuation?:  1\n",
      "\n",
      "\n",
      "Doc after removing stopwords and punctuation\n",
      "love\n",
      "read\n",
      "book\n",
      "beach\n",
      "breeze\n",
      "relaxing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Parse text through the `nlp` model\n",
    "text = \"\"\"I love to read a book on a beach. The breeze is relaxing!\"\"\"\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(text)\n",
    "# Printing tokens and boolean values stored in different attributes\n",
    "for token in doc:\n",
    "    print(f'Token Text: {token.text:10} Stop word?: {token.is_stop:2} Punctuation?: {token.is_punct:2}')\n",
    "    \n",
    "# Removing StopWords and punctuations\n",
    "doc_cleaned = [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "print('\\n\\nDoc after removing stopwords and punctuation')\n",
    "for token in doc_cleaned:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb5f35",
   "metadata": {},
   "source": [
    "#### Identifying words which do not have a valid POS tag (POS = X) \n",
    "Ssuch words as etc, i.e. are assigned POS = X by spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "412f5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before preprocessing:  13\n",
      "\n",
      "Now preprocessing to remove words with POS == X, stopwords, and punctuation...\n",
      "\n",
      "No. of tokens after removing stopwords and punctuation:  7\n",
      "\n",
      "Cleaned Doc:\n",
      "Token Text:26          POS: NUM     \n",
      "Token Text:books       POS: NOUN    \n",
      "Token Text:$           POS: SYM     \n",
      "Token Text:10.5        POS: NUM     \n",
      "Token Text:plus        POS: CCONJ   \n",
      "Token Text:10          POS: NUM     \n",
      "Token Text:tax         POS: NOUN    \n"
     ]
    }
   ],
   "source": [
    "# Identifying all POS tags for tokens in a piece of text.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text= \"\"\"He 26 books. $ 10.5 plus 10% tax etc etc. \"\"\"\n",
    "\n",
    "# Pass the Text to Model\n",
    "doc = nlp(text)\n",
    "\n",
    "print('Number of tokens before preprocessing: ', len(doc))\n",
    "\n",
    "print(\"\\nNow preprocessing to remove words with POS == X, stopwords, and punctuation...\")\n",
    "\n",
    "cleaned_doc = [token for token in doc if not token.pos_=='X' and not token.is_stop and not token.is_punct]\n",
    "\n",
    "print('\\nNo. of tokens after removing stopwords and punctuation: ', len(cleaned_doc))\n",
    "print('\\nCleaned Doc:')\n",
    "for token in cleaned_doc:\n",
    "    print(f'Token Text:{token.text:10}  POS: {token.pos_:8}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8f186",
   "metadata": {},
   "source": [
    "#### Printing the tokens which are like numbers or email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b47d75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A number: 2020\n",
      "A number: 10\n",
      "A percentage number: 10\n",
      "An email address:  jhucasualties@ada.com\n"
     ]
    }
   ],
   "source": [
    "#Printing the tokens which are like numbers or email addresses\n",
    "text= \"\"\"Covid19 made 2020 a catastropic year, 10% people who got infected died. \n",
    "       All casualties were reported to jhucasualties@ada.com\"\"\"\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        print(\"A number:\", token)\n",
    "    \n",
    "        index_of_next_token=token.i + 1\n",
    "        next_token = doc[index_of_next_token]\n",
    "        if next_token.text == '%':\n",
    "            print(\"A percentage number:\", token.text)\n",
    "    \n",
    "    \n",
    "    if token.like_email:\n",
    "        print(\"An email address: \", token.text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e2837d",
   "metadata": {},
   "source": [
    "#### Morphology \n",
    "In a natural language, a lemma (root form) of a word is inflected (modified/combined) with one or more morphological features to create many froms of the root form. Here are some examples:\n",
    "\n",
    "root form : read\n",
    "inflected forms : reading, read\n",
    "\n",
    "Morphological features are stored in the MorphAnalysis under Token.morph, which allows you to access individual morphological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a557e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Aspect=Prog|Tense=Pres|VerbForm=Part\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I was reading the newspaper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  \n",
    "\n",
    "\n",
    "doc = nlp(\"She is reading the paper.\")\n",
    "token = doc[2]  # 'reading'\n",
    "print(token.morph)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627041a",
   "metadata": {},
   "source": [
    "#### Hashes of Strings in document and their look up in StringStore\n",
    "- You can print the hash value of a token or its string text from Vocab - **nlp.vocab.strings**.\n",
    "\n",
    "#### Vocab\n",
    "\n",
    "- **Vocab** is a storage class for vocabulary and other data shared across a language.\n",
    "- The Vocab object provides a lookup table to access Lexeme objects and the StringStore. \n",
    "- You can see all words in vocabulary of a langauage model using this statement: \n",
    "\n",
    " <code> list(nlp.vocab.strings) </code>\n",
    "\n",
    "#### StringStore class\n",
    "This class allows you to look up strings their by 64-bit hashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "043d85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash of the word 'murder' is 730091238969817242\n",
      "Word corresponding to the hash 730091238969817242 is 'murder''\n"
     ]
    }
   ],
   "source": [
    "# Print hash and the corresponfing token text\n",
    "doc = nlp(\"Sherlock Holmes solved the murder mystery like a piece of cake\")\n",
    "\n",
    "# Look up the hash for the word \"murder\" in nlp.vocab.strings\n",
    "hash = nlp.vocab.strings[\"murder\"]\n",
    "print(\"hash of the word 'murder' is {}\".format(hash))\n",
    "\n",
    "# Look up the word using a hash value in nlp.vocab.strings \n",
    "word = nlp.vocab.strings[hash]\n",
    "print(\"Word corresponding to the hash {} is '{}''\".format(hash, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b145903",
   "metadata": {},
   "source": [
    "#### [Named Entity Recognition](https://spacy.io/usage/linguistic-features#named-entities-101)\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy's trained language pipeline models can predict various types of named entities in a document.\n",
    "all the named entities are available as the **ents** attribute of of a **Doc** object. \n",
    "\n",
    "The common Named Entity categories supported by spacy are :<br>\n",
    "\n",
    "PERSON : Names of people<br>\n",
    "GPE : Geo physical entity - like counties, cities, states<br>\n",
    "ORG : Organizations or companies<br>\n",
    "WORK_OF_ART : Titles of books, fimls,songs and other arts<br>\n",
    "PRODUCT : Products such as vehicles, food items ,furniture and so on<br>\n",
    "EVENT : Historical events like wars, disasters ,etc <br>\n",
    "LANGUAGE : All the recognized languages across the globe<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f33a2d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### See all entity types in  specific langauge model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60698c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entities in this doc (Google, Samsung, Microsoft, John, Walmart)\n",
      "\n",
      "All the companies: \n",
      "Google\n",
      "Samsung\n",
      "Microsoft\n",
      "Walmart\n",
      "\n",
      "\n",
      "All the people: \n",
      "John\n"
     ]
    }
   ],
   "source": [
    "# Identofy all the entities that are ORG in given text\n",
    "text = \"\"\" This is an important industrial region of the country. \n",
    "           It houses the offices of Google, Samsung, Microsoft etc.\n",
    "           Mr. John works in Walmart .\"\"\"\n",
    "\n",
    "# creating spacy doc\n",
    "doc = nlp(text)\n",
    "\n",
    "# Printing the named entities\n",
    "print('All entities in this doc', doc.ents)\n",
    "\n",
    "\n",
    "print(\"\\nAll the companies: \")\n",
    "for entity in doc.ents:\n",
    "    if entity.label_=='ORG':\n",
    "        print(entity.text)\n",
    "\n",
    "        \n",
    "print(\"\\n\\nAll the people: \")        \n",
    "for entity in doc.ents:\n",
    "    if entity.label_== 'PERSON':\n",
    "        print(entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caa20c",
   "metadata": {},
   "source": [
    "#### [Masking certain entity types and tokens representing personal information for privacy](https://www.machinelearningplus.com/spacy-tutorial-nlp/#application2automaticallymaskingentities)\n",
    "Here in the code below, we mask the token that have entity type PERSON , ORG, GPE , PRODUCT or EVENT, and tokens that are numbers or email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "56797606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All entities in this doc (John, Italy, Google, 123456)\n",
      "\n",
      "Identified named entities: \n",
      "Entity: John       Entity Label: 380 \n",
      "Entity: Italy      Entity Label: 384 \n",
      "Entity: Google     Entity Label: 383 \n",
      "Entity: 123456     Entity Label: 391 \n",
      "\n",
      "Masked Personal Identification Information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi , my name is Dr. UNKNOWN . I live in UNKNOWN . I work at UNKNOWN . My phone number is UNKNOWN . My email is UNKNOWN'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Source: https://www.machinelearningplus.com/spacy-tutorial-nlp/#application2automaticallymaskingentities\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"\"\"Hi, my name is Dr. John. I live in Italy. I work at Google. My phone number is 123456. My email is abcd@abcd.com\"\"\"\n",
    "\n",
    "# creating spacy doc\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "\n",
    "print('All entities in this doc', doc.ents)\n",
    "\n",
    "print(\"\\nIdentified named entities: \")\n",
    "for ent in doc.ents:\n",
    "    print(\"Entity: {:<10} Entity Label: {:<4}\".format(ent.text, ent.label))\n",
    "\n",
    "def find_personaldata(token):\n",
    "    if token.ent_type_ == 'PERSON' or token.ent_type_== 'ORG' or token.ent_type_== 'GPE' or token.like_num or token.like_email:\n",
    "        return 'UNKNOWN'\n",
    "    return token.text\n",
    "\n",
    "def mask_personaldata(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(find_personaldata, doc)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print('\\nMasked Personal Identification Information')\n",
    "mask_personaldata(doc)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8852c",
   "metadata": {},
   "source": [
    "### Rule based Matching\n",
    "\n",
    "- spaCy allows you to find and extract tokens and Docs that match a specific text pattern or have certain lexical attributes through use of a rule based matching engine - called Matcher. <br>\n",
    "- For example, find the word \"duck\" only if it's a verb, not a noun.<br>\n",
    "- You can write your own rules. <br>\n",
    "- spacy supports these kinds of matching methods: Token Matcher, Phrase Matcher, Entity Ruler, Token Matcher <br>\n",
    "\n",
    "#### Example rules for pattern matching in the form of lists of dictionaries, one per token:<br>\n",
    "\n",
    "- Match exact token texts    : [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]<br>\n",
    "- Match lexical attributes   : [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]<br>\n",
    "- Match any token attributes : [{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}] , [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]<br>\n",
    "\n",
    "\n",
    "#### The procedure to implement a token matcher is:\n",
    "\n",
    "1. Initialize a Matcher object<br>\n",
    "2. Define the pattern you want to match<br>\n",
    "3. Add the pattern to the matcher<br>\n",
    "4. Pass the text to the matcher to extract the matching positions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "da34e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Hello, world\n",
      "7 10 Hello, world\n"
     ]
    }
   ],
   "source": [
    "#Source: https://spacy.io/usage/rule-based-matching\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"mypattern\", [pattern])\n",
    "\n",
    "text = \"Hello, world! Hello world! Hello, world\"\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # match_id is the hash value of matched string, here we get the string representation of this hash value\n",
    "    span = doc[start:end]  # The matched span within the doc\n",
    "    print(start, end, span.text) # print start token position,  end token position and matches span of each match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6536cd3",
   "metadata": {},
   "source": [
    "###  Visualizers - displaCy and displaCy ENT(https://spacy.io/usage/visualizers)\n",
    "#### using [displacy](https://explosion.ai/demos/displacy)\n",
    "\n",
    "- You can easily visualize dependencies identified by a langauge models in your text using two spaCy visualizers - displacy and displaCy Named Entity Visualizer [displaCy ENT](https://explosion.ai/demos/displacy-ent).\n",
    "- Both displaCy and displaCy ENT are part of the core spaCy library. \n",
    "- displaCy can either take a single Doc or a list of Doc objects as its first argument.\n",
    "- displaCy can return the markup that can be rendered in jupyter notebook or exported. \n",
    "- displaCy.serve creates a simple web server and lets you view your visualization in a browser. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7c69173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"10788a03949844aab6d7eeb57efc3be9-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">read</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-10788a03949844aab6d7eeb57efc3be9-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-10788a03949844aab6d7eeb57efc3be9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-10788a03949844aab6d7eeb57efc3be9-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-10788a03949844aab6d7eeb57efc3be9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-10788a03949844aab6d7eeb57efc3be9-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-10788a03949844aab6d7eeb57efc3be9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying dependency relations between tokens\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = 'I love to read'\n",
    "doc = nlp(text)\n",
    "displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643ce37",
   "metadata": {},
   "source": [
    "#### displacy.serve visualizing [dependency parsing](https://spacy.io/usage/visualizers#dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4452fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\NimritaAnaconda\\envs\\myenv\\lib\\site-packages\\spacy\\displacy\\__init__.py:103: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6c98f941f89e4ea6abdaf29389a24977-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Some</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">neighbors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">live</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">three</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">adjacent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">houses.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c98f941f89e4ea6abdaf29389a24977-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c98f941f89e4ea6abdaf29389a24977-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "#visualizing dependency parsing\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"Some neighbors live in three adjacent houses.\"\"\"\n",
    "doc = nlp(text)\n",
    "sentence_spans = list(doc.sents)\n",
    "displacy.serve(sentence_spans, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bee4df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working at Google in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "# Visualizing named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"Sebastian Thrun started working at Google in 2007\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ca47c",
   "metadata": {},
   "source": [
    "## [Next we will train the Named Entity Recognizer Component of the pipeline for a new entity called \"Gadget\"](https://course.spacy.io/en/chapter4)\n",
    "\n",
    "### Why update the model?\n",
    "Better results on your specific domain\n",
    "Learn classification schemes specifically for your problem\n",
    "Essential for text classification\n",
    "Very useful for named entity recognition\n",
    "Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "### Example: Training the entity recognizer that recognises iphones as entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0736da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Source : https://course.spacy.io/en/chapter4\n",
    "# https://github.com/explosion/spacy-course/tree/master/exercises/en\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# You can access the files at https://raw.githubusercontent.com/explosion/spacy-course/master/exercises/en/iphone.json\n",
    "with open(\"iphone.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and create docs with matched entities\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "91148ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f03acb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the DocBin with the list of docs.\n",
    "#Save the DocBin to a file called train.spacy.\n",
    "# Create and save a collection of training docs\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"C:/spacyfiles/train.spacy\")\n",
    "# Create and save a collection of evaluation docs\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"C:/spacyfiles/dev.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1474bcd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "After this we need to generate a config.cfg file with spacy init config command. <br>\n",
    "For this, use this line of code at your Anaconda prompt. <br>\n",
    "Issue the command in the same environment where you have installed spacy. <br>\n",
    "    \n",
    "<code>python -m spacy init config ./config.cfg --lang en --pipeline ner </code><br>\n",
    "    \n",
    "init config: the command to run<br>\n",
    "config.cfg: output path for the generated config<br>\n",
    "--lang: language class of the pipeline, e.g. en for English<br>\n",
    "--pipeline: comma-separated names of components to include<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0a448",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next we train the model by running following command at Anaconda prompt in the environment in which you have installed spacy\n",
    "<br>\n",
    "\n",
    "<code>python -m spacy train config.cfg --output C:/spacymodeloutput  --paths.train  C:/spacyfiles/train.spacy  --paths.dev C:/spacyfiles/dev.spacy </code><br>\n",
    "\n",
    "      \n",
    "train: the command to run<br>\n",
    "config.cfg: the path to the config file<br>\n",
    "--output: the path to the output directory to save the trained pipeline<br>\n",
    "--paths.train: override with path to the training data, provide the path of train.spacy<br>\n",
    "--paths.dev: override with path to the evaluation data, provide the path of dev.spacy<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bae7a",
   "metadata": {},
   "source": [
    "## During training\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/NimritaKoul/IntroductiontoNLPwith_spaCy/main/TrainingNERwithSpacy.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b237e",
   "metadata": {},
   "source": [
    "### Loading a trained pipeline\n",
    "output after training is a regular loadable spaCy pipeline\n",
    "model-last: last trained pipeline\n",
    "model-best: best trained pipeline\n",
    "load it with spacy.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e13a0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iPhone 11, iPhone 8)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"C:/spacymodeloutput/model-best\")\n",
    "doc = nlp(\"iPhone 11 is better than iPhone 8\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91693fce",
   "metadata": {},
   "source": [
    "### Packaging your pipeline\n",
    "spacy package: creates an installable Python package containing your pipeline, easy to version and deploy\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "To create a distributable package of your newly trained model use this command at the Ananconda prompt in the same environment in which you installed spacy.\n",
    "\n",
    "<code>python -m spacy package C:/spacymodeloutput/model-best C:/myspacypackages --name my_pipeline --version 1.0.0</code><br>\n",
    "Here :<br>\n",
    "    spacy package: is the command that creates a package<br>\n",
    "    c:/spacymodeloutput/model-best : is the path of the folder which contains best model generated in training process<br>\n",
    "    c:/myspacypacakges : is the path in which I want the newly created package to be stored<br>\n",
    "    my_pipeline : is the name I wish to be given to the created pipeline<br>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef00b62",
   "metadata": {},
   "source": [
    "### Installing your own pipeline into your Jupyter Notebook\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "At the Anaconda Prompt (in your spacy environment) first change the directory to one where your pacakage is stored.<br>\n",
    "\n",
    "<code>cd C:/myspacypackages/en_my_pipeline-1.0.0</code><br>\n",
    "\n",
    "Then at the same prompt, use pip install to install the pipeline from tar file.<br>\n",
    "\n",
    "<code>pip install dist/en_my_pipeline-1.0.0.tar.gz</code><br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e2c8d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iPhone 11, iPhone 8)\n"
     ]
    }
   ],
   "source": [
    "### Load and use your own pipeline\n",
    "my_nlp_pipeline = spacy.load(\"en_my_pipeline\")\n",
    "doc = my_nlp_pipeline(\"iPhone 11 is better than iPhone 8:\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58866531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
